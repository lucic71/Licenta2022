\chapter{Discussions on Porting Costs} \label{sec:discussion}

% What I want to say in this section:
%   - I want to reflect on imporving the porting process we did. Tanaka proposes
%   the following seven ways of raising porting efficiency:
%     - porting guidlines
%     - porting compatibility checking tool
%     - portability evaluation tool
%     - tool for generating system calling routines
%     - program structure viewing tool
%     - os emulator
%     - test support tools
%   - I would like to see how relevant are they for our project. there is no way
%   at the moment to actually test the ways of improving porting efficiency so
%   I will keep the statements at the level of discussion, thinking about how
%   would have the process been different if we used one of the above ways (this
%   is chapter 3.2 from Tanaka)

% Subjects for discussion:
%  - the porting tasks affect each other in a non linear fashion
%  - we had errors in our extraction of costs from tracking
%  - it would be nice to have a dependency graph for the porting tasks. however
%  we could try to sketch some dependencies and see if an imporvement on one
%  side affects the other side
%  - what where the limitations in implementations and how do the improvements
%  from Tanaka apply to them
%  - compare our porting costs with the Table from Tanaka
%
%  - what does it mean to have an universal computer interface that would remove
%  any porting whatsoever (this discussion is in ../src -> background section if
%  i remember correctly)

In this chapter we discuss some conclusions of our porting work including the
limitations of the porting model we used and factors for these limitations,
namely the dependency between porting tasks. We also compare our results with
the results of the first paper that published a porting model to understand what
were the differences between our and their results and conclusions. Finally, we
also present general difficulties that we extracted from our porting.

\section{Conclusions of our porting}

In this section we present observations about the porting process and the
porting costs, focusing on the limitations of the porting model we used and on a
comparison between our results and the results of Tanaka et al.

\subsection{Limitations of the porting model}

The model assumes that the tasks are executed in
sequential order, which is not true. The tasks are rather executed in a
non-linear fashion. For example while building for the target environment,
installing the binaries in this environment and testing the ported application,
we also had discussions about the difficulties and errors we encountered so that
we could later come back and solve the inconsistencies. However there was no
easy way to describe this dynamic so we chose to represent the task as they
would come one after another. The non-linearity of this porting model causes
problems while computing the porting costs. If one wants to compute the costs
with minimum error it means that more time must be allocated to the
\textit{Progress tracking} subtask. This strategy might not bring the best
results if the fixed total time of porting is computed in advance because
allocating more time for progress tracking means that other porting tasks must
have lesser time allocated. A good strategy here would compute first the
accepted error of man-hours present in the final porting costs so that an
acceptable ammount of time is allocated for tracking the progress of the
project. For our porting process it was not vital to have a small error in the
porting costs, we were interested to see an approximative distribution of time
per tasks so that we could answer questions as: where did we spend most time and
why? or what was the relation between development and testing? For this we
allocated an hour each week for tracking the status of the project, the planned
objectives for the future and the major difficulties we met in the respective
week. It is unclear, however, if more time spent on progress tracking would
improve the quality of other tasks. We can only guess, but more time spent on
tracking the working hours would mean that the developer has a better sense of
the time spent on each task, thus better managing multiple tasks at a time in
the future.

As discussed above, we had errors while extracting the porting costs for each
subtask in Table~\ref{tab:manHoursEvaluation}. There are two reasons for this
issue: the progress tracking format was not descriptive enough and the model we
used for extracting the porting costs had shortcomings. Firstly, the progress
tracking format should have an intrinsic expresiveness that would make it easy
for the user to express the non-linearity of the porting tasks. To create this
kind of format we need to first find a way to describe the non-linearity of
the porting process, which is not a trivial task whatsoever.  Secondly, we
expect a conversion error from the classical progress tracking format (i.e.,
list of bullets for current status and planning) to the porting model
presented in section~\ref{sec:background}. This happens because there is no
straightforward way of converting a bullet as "IM is booting, having config
issues" (this example is taken from our progress tracking) into clear and
independent porting tasks. This is subject to interpretation one might say
that this bullet might be broken down into the following porting tasks:
\begin{itemize}
        \item Reviewing inconsistencies between source and remote environments
        \item Testing in target environment
        \item Discussions
        \item Surveying target OS
\end{itemize}
while another would break the bullet down into the following porting tasks:
\begin{itemize}
        \item Reviewing inconsistencies between source and remote environments
        \item Testing in target environment
\end{itemize}
It very much depends on the human factors and how the developers decide to work
with the provided tools and systems. Furthermore, to have a more accurate
conversion there must be time considerents: how much time did we spent on this
bullet? and how do we divide this time between porting tasks?

\lm{If I understand correctly, the problem here is one of taxonomy: a given task could fall into more than one category, or rather, the time allocated to that task could be broken between multiple categories. I'm wondering, then: do the ``porting task categories'' themselves represent the porting process? Or is there some better way to break down porting processes (in general)?}
\lp{I don't know how to answer that question}

\subsection{Dependencies between porting tasks}

The next discussion point focuses on the porting tasks dependency graph. It
would be very difficult to sketch a complete graph between all tasks, instead we
can focus on specific areas of the graph that are somehow easier to understand
and sketch. The first area is the dependency between \textit{Testing} and all
subtasks in \textit{Building for target environment}. There is a continous
feedback between the testing phase and the development phase. While porting we
built binaries, installed them on target environment, tested them and expected
to see errors of linking, problems with the installation or actual
inconsistencies and problems with dependencies that would be solved in a future
iteration of this process. This iterative process with continous feedback
between testing and development is a building block of the Agile methodology, if
we used other software methodology as Waterfall maybe the dependency would be
completely different. The second area we focus on is the dependency between
\textit{General duties} and all other subtasks. Documentation is linked to all
subtasks because we need to include all relevant information in the documents we
produce, progress tracking must reflect work executed in each subtask and
discussions are by design focused on trying to understand each part of the
porting process. That being said, it is hard to imagine how \textit{General
duties} would not be a substantial part of the porting process. It would be
interesting to see if a modification in the allocated time for a subtask such
as \textit{Discussions} would reduce or increase the needed time for other
subtasks, otherwise if other tasks are not affected by this modification, there
should be no interest in allocating more time for discussing.

\section{Comparison between our results and the results of Tanaka et al.}

At this moment we discussed the limitations and open problems that appear in the
model of porting we used. Next, we make a comparison between our results for
porting costs and the results that appeared in the first paper that presented a
basis for our porting model~\cite{tanaka}. The results can be found in Table 6 of
the paper we referenced. We will compare the subtotals per tasks and discuss the
reasons of the differences between the two. In terms of \textit{Advance
preparations} we spend a total of 14.35\% of our total time while Tanaka et al.
spend 33.4\%. There are two major reasons for reduced time in our case. First,
we were interested in delivering builds for the target environment as fast as
possible, sacrificing the time spent on understanding each part of the system we
were trying to port. We found no easy way to understand the components of the
system by surveying the documentation and the program for porting so we decided
to offload this work on the build system (i.e., when the compiler threw errors
for a component, we went to that component, understood it and solved the
problems). Second, we worked with the Agile methodology in mind, combining
subtasks from different tasks in the same iteration. For this reason we
lost the focus on the \textit{Advance preparations} because we were also
interested in subtasks from \textit{Building for target environment}, for
example. Tanaka et al. seem to work with a Waterfall methodology. This might
explain the increased time in this initial stage.

Next we compare the \textit{Building for target environment} in our model with
\textit{Target testing} from Tanaka et al. There are significant differences in
the subtasks from the two models but we are interested in a comparison of
development time (i.e., actual work focused on solving errors and
inconsistencies, building binaries, etc.). It seems that Tanaka et al. put the
time spent on solving errors and the time spent on testing in the same category.
This makes the comparison difficult as we do not know how to divide this time.
However our time spent in this stage is 32.71\% while Tanaka et al. spend
27.8\%. The most time consuming task for us was to work with the build system,
which is not the case for them. This shows the major difference between our
project and theirs. While our porting was focused on extracting components from
a larger ecosystem into a target environment, their project was focused on
porting a whole application from one environment to another.

For \textit{Testing} we assume that they spent $\textit{Linked test on target} /
2 + \textit{Workstation testing} = 10.5 + 11.4 = 21.9\%$. We divided the linked
test on target because we assumed that half of this time was allocated to
development and half of it was allocated to testing. Their time for
\textit{General duties} is of 27.4\%. These two times are very similar with our
project (i.e, 24.53\% and 28.26\%). This might mean that testing and general
duties represent a major part in each software porting project, thus when
evaluating the costs in advance, special attention must be payed to these tasks.

\section{General difficulties}

This section presentes the general impediments we faced during our porting. The
difficulties were extracted from the parcitular technical difficulties we faced
during the porting process.

\subsection{Lacking documentation}
Lacking written documentation about how the system works means
that the developer must either figure out the system alone or must communicate
with other developers in order to gather information about the system. This adds
overhead to the porting process as documentation through communication is
slower than documentation through written text.

\subsection{Inconsistencies between environments}
This difficulty corresponds to the degree of which the application is
portable~\cite{mooney2004developing} between two given environments. If the
degree of portability is too low (this depends entirely on the application) then
the developer will be faced with many inconsistencies between the source and
target environments that will be reflected in the cost of porting (i.e.,
man-hours).

\subsection{Use of tools}
Using inadequate development and testing tools introduces additional
overhead in the porting process. This happens when the used tools are either
outdated or too hard to use~\lm{I'm still not bought on the ``outdated'' aspect; developers shouldn't suffer just because a maintainer decides to break compatibility with an older version of a tool.} 
\lp{is there something we can do regarding this?}
for the purpose of the process.

\subsection{Understanding the system}
Software complexity is a multi-dimensional problem, it includes: structural,
computational, logical, conceptual and textual complexity~\cite{ejiogu}. There
is no easy way to understand the system, so the developer will be faced with the
task of understanding the architecture diagrams, huge code base, written
documents and tutorials either when the porting starts or during the porting
process.

\subsection{Unexpected difficulties}
As with every software process, there are difficulties
that can not be estimated beforehand. They include unknown parts of the software
that may cause problems when ported or assumptions about the environment made by
the system that will not work in the target environment. The only time they
are clearly reflected in the porting costs is at the end of the project where
a retrospective is led.~\lm{I know I commented at least twice in the past on this paragraph and I'm still unconvinced about it. One needs to look deeper into what this ``unexpected'' means, or rather, what causes it. To my eye, something is always unexpected in the context of partial information, so ``Unexpected difficulties'' is not disjoint from ``Understanding the system''. You have to ask yourself: had you known more things beforehand (e.g. architectural details about IxOS, or RPi delivery date), would the unexpected difficulties still be unexpected? If not, then there's really nothing ``unexpected'' about them, they were just caused by lack of familiarity with the system.}
